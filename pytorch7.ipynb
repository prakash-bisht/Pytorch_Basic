{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uXrZv120Kidq",
        "jkFX4qDhGdnh"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakash-bisht/Pytorch_Basic/blob/master/pytorch7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#multi-gpu & multi-machine"
      ],
      "metadata": {
        "id": "uXrZv120Kidq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile datautils.py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyTrainDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HVOC4t6qgTN",
        "outputId": "2fe978d2-3a69-4c33-982a-1c7620d435b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing datautils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyTrainDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]"
      ],
      "metadata": {
        "id": "wPSPwKAw1_it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40xEO5KDk2Te",
        "outputId": "d1940d2b-8aac-4a2f-e5ca-c36e3392a728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting single_gpu.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile single_gpu.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int, \n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "\n",
        "def main(device, total_epochs, save_every, batch_size):\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, device, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    device = 0  # shorthand for cuda:0\n",
        "    main(device, args.total_epochs, args.save_every, args.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int, \n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "\n",
        "def main(device, total_epochs, save_every, batch_size):\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, device, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    device = 0  # shorthand for cuda:0\n",
        "    main(device, args.total_epochs, args.save_every, args.batch_size)"
      ],
      "metadata": {
        "id": "W3l5asb12EaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python single_gpu.py 20 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXJx9kVfk3jA",
        "outputId": "5ffed8e4-91ad-4a06-f312-af39d2a33f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU0] Epoch 0 | Batchsize: 32 | Steps: 64\n",
            "Epoch 0 | Training checkpoint saved at checkpoint.pt\n",
            "[GPU0] Epoch 1 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 2 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 3 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 4 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 5 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 6 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 7 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 8 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 9 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 10 | Batchsize: 32 | Steps: 64\n",
            "Epoch 10 | Training checkpoint saved at checkpoint.pt\n",
            "[GPU0] Epoch 11 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 12 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 13 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 14 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 15 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 16 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 17 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 18 | Batchsize: 32 | Steps: 64\n",
            "[GPU0] Epoch 19 | Batchsize: 32 | Steps: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multi_gpu.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup(rank, world_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rank: Unique identifier of each process\n",
        "        world_size: Total number of processes\n",
        "    \"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.model = DDP(model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.module.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkcbUqAYk3tN",
        "outputId": "49f2e908-009d-4306-abab-a3f171ef358b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multi_gpu.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup(rank, world_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rank: Unique identifier of each process\n",
        "        world_size: Total number of processes\n",
        "    \"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.model = DDP(model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.module.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)"
      ],
      "metadata": {
        "id": "wC_rri6j2JVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multigpu-torchrun.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup():\n",
        "    init_process_group(backend=\"nccl\")\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        save_every: int,\n",
        "        snapshot_path: str,\n",
        "    ) -> None:\n",
        "        self.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n",
        "        self.model = model.to(self.gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.epochs_run = 0\n",
        "        self.snapshot_path = snapshot_path\n",
        "        if os.path.exists(snapshot_path):\n",
        "            print(\"Loading snapshot\")\n",
        "            self._load_snapshot(snapshot_path)\n",
        "\n",
        "        self.model = DDP(self.model, device_ids=[self.gpu_id])\n",
        "\n",
        "    def _load_snapshot(self, snapshot_path):\n",
        "        loc = f\"cuda:{self.gpu_id}\"\n",
        "        snapshot = torch.load(snapshot_path, map_location=loc)\n",
        "        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n",
        "        self.epochs_run = snapshot[\"EPOCHS_RUN\"]\n",
        "        print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_snapshot(self, epoch):\n",
        "        snapshot = {\n",
        "            \"MODEL_STATE\": self.model.module.state_dict(),\n",
        "            \"EPOCHS_RUN\": epoch,\n",
        "        }\n",
        "        torch.save(snapshot, self.snapshot_path)\n",
        "        print(f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(self.epochs_run, max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_snapshot(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str = \"snapshot.pt\"):\n",
        "    ddp_setup()\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    main(args.save_every, args.total_epochs, args.batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca3U3g5xk3y3",
        "outputId": "2251ece5-5927-445a-c025-32d97cfdcf21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multigpu-torchrun.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup():\n",
        "    init_process_group(backend=\"nccl\")\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        save_every: int,\n",
        "        snapshot_path: str,\n",
        "    ) -> None:\n",
        "        self.gpu_id = int(os.environ[\"LOCAL_RANK\"])\n",
        "        self.model = model.to(self.gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.epochs_run = 0\n",
        "        self.snapshot_path = snapshot_path\n",
        "        if os.path.exists(snapshot_path):\n",
        "            print(\"Loading snapshot\")\n",
        "            self._load_snapshot(snapshot_path)\n",
        "\n",
        "        self.model = DDP(self.model, device_ids=[self.gpu_id])\n",
        "\n",
        "    def _load_snapshot(self, snapshot_path):\n",
        "        loc = f\"cuda:{self.gpu_id}\"\n",
        "        snapshot = torch.load(snapshot_path, map_location=loc)\n",
        "        self.model.load_state_dict(snapshot[\"MODEL_STATE\"])\n",
        "        self.epochs_run = snapshot[\"EPOCHS_RUN\"]\n",
        "        print(f\"Resuming training from snapshot at Epoch {self.epochs_run}\")\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_snapshot(self, epoch):\n",
        "        snapshot = {\n",
        "            \"MODEL_STATE\": self.model.module.state_dict(),\n",
        "            \"EPOCHS_RUN\": epoch,\n",
        "        }\n",
        "        torch.save(snapshot, self.snapshot_path)\n",
        "        print(f\"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(self.epochs_run, max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_snapshot(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str = \"snapshot.pt\"):\n",
        "    ddp_setup()\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    main(args.save_every, args.total_epochs, args.batch_size)"
      ],
      "metadata": {
        "id": "-jfd39XY2SY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile multi_gpu_multi_machine.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup(rank, world_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rank: Unique identifier of each process\n",
        "        world_size: Total number of processes\n",
        "    \"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.model = DDP(model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.module.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqsKtNjQttNu",
        "outputId": "0a78292c-8b25-421d-e81e-cc4edfd53ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing multi_gpu_multi_machine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For multi machine config visit--\n",
        "#https://github.com/pytorch/examples/tree/main/distributed/ddp-tutorial-series"
      ],
      "metadata": {
        "id": "h_q9eFiPES2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datautils import MyTrainDataset\n",
        "\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import os\n",
        "\n",
        "\n",
        "def ddp_setup(rank, world_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        rank: Unique identifier of each process\n",
        "        world_size: Total number of processes\n",
        "    \"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
        "    init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        train_data: DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        gpu_id: int,\n",
        "        save_every: int,\n",
        "    ) -> None:\n",
        "        self.gpu_id = gpu_id\n",
        "        self.model = model.to(gpu_id)\n",
        "        self.train_data = train_data\n",
        "        self.optimizer = optimizer\n",
        "        self.save_every = save_every\n",
        "        self.model = DDP(model, device_ids=[gpu_id])\n",
        "\n",
        "    def _run_batch(self, source, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        output = self.model(source)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _run_epoch(self, epoch):\n",
        "        b_sz = len(next(iter(self.train_data))[0])\n",
        "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
        "        self.train_data.sampler.set_epoch(epoch)\n",
        "        for source, targets in self.train_data:\n",
        "            source = source.to(self.gpu_id)\n",
        "            targets = targets.to(self.gpu_id)\n",
        "            self._run_batch(source, targets)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        ckp = self.model.module.state_dict()\n",
        "        PATH = \"checkpoint.pt\"\n",
        "        torch.save(ckp, PATH)\n",
        "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
        "\n",
        "    def train(self, max_epochs: int):\n",
        "        for epoch in range(max_epochs):\n",
        "            self._run_epoch(epoch)\n",
        "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "\n",
        "def load_train_objs():\n",
        "    train_set = MyTrainDataset(2048)  # load your dataset\n",
        "    model = torch.nn.Linear(20, 1)  # load your model\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "    return train_set, model, optimizer\n",
        "\n",
        "\n",
        "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        pin_memory=True,\n",
        "        shuffle=False,\n",
        "        sampler=DistributedSampler(dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):\n",
        "    ddp_setup(rank, world_size)\n",
        "    dataset, model, optimizer = load_train_objs()\n",
        "    train_data = prepare_dataloader(dataset, batch_size)\n",
        "    trainer = Trainer(model, train_data, optimizer, rank, save_every)\n",
        "    trainer.train(total_epochs)\n",
        "    destroy_process_group()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='simple distributed training job')\n",
        "    parser.add_argument('total_epochs', type=int, help='Total epochs to train the model')\n",
        "    parser.add_argument('save_every', type=int, help='How often to save a snapshot')\n",
        "    parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    world_size = torch.cuda.device_count()\n",
        "    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)"
      ],
      "metadata": {
        "id": "lKIMGONhudU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory"
      ],
      "metadata": {
        "id": "jkFX4qDhGdnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient accumulation-----------is a way to train a machine learning model with more data at once than the computer's memory can handle. \n",
        "                                  # Instead of training on a large batch of data all at once, the training is split into smaller batches,\n",
        "                                  # and the gradients (the directions in which the model needs to adjust its weights to improve) are saved after each batch. \n",
        "                                  # After several batches, the gradients are combined and the optimizer is then updated with the combined gradients. \n",
        "                                  # This allows the model to train on more data than it would be able to in one go, without running out of memory."
      ],
      "metadata": {
        "id": "Tdn0D38DudkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_linear_schedule_with_warmup-------------Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, \n",
        "#                                             after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer."
      ],
      "metadata": {
        "id": "VDbColy_EQ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='/content/warmup_linear_schedule.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "gfNACe-fEQ2w",
        "outputId": "168651e3-f3d0-4bbe-9c7c-7eba2d19fba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}