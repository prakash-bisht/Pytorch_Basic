{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5LBzrnmabNND",
        "1c8qFopqbCPG"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakash-bisht/Pytorch_Basic/blob/master/pytorch6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATALOADERS"
      ],
      "metadata": {
        "id": "5LBzrnmabNND"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPzj6a5WfL2W",
        "outputId": "8ead2836-e63e-45fc-9397-16f7d1003576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xs values:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "ys values:  [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
          ]
        }
      ],
      "source": [
        "xs = list(range(10))\n",
        "ys = list(range(10,20))\n",
        "print('xs values: ', xs)\n",
        "print('ys values: ', ys)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = list(zip(xs,ys))\n",
        "dataset[0] # returns the tuple (x[0], y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXfl-3WJfS7t",
        "outputId": "2d9d8b0b-645f-4f9a-b249-5e04623e0f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset:\n",
        "    def __init__(self, xs, ys):\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.xs[i], self.ys[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.xs)"
      ],
      "metadata": {
        "id": "DZerK0LBfTGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset(xs, ys)\n",
        "dataset[2] # returns the tuple (x[2], y[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frfB5uR-fTIj",
        "outputId": "db153c94-a36d-4f49-e293-86bae2b9c74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "for x, y in DataLoader(dataset):\n",
        "    print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMcAGVuFfTOv",
        "outputId": "fcc06518-f922-4c25-9125-b1342a3a0392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0]) tensor([10])\n",
            "tensor([1]) tensor([11])\n",
            "tensor([2]) tensor([12])\n",
            "tensor([3]) tensor([13])\n",
            "tensor([4]) tensor([14])\n",
            "tensor([5]) tensor([15])\n",
            "tensor([6]) tensor([16])\n",
            "tensor([7]) tensor([17])\n",
            "tensor([8]) tensor([18])\n",
            "tensor([9]) tensor([19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in DataLoader(dataset, batch_size=2):\n",
        "    print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIHIWH9KfTS8",
        "outputId": "6dac9d23-e175-489a-ec81-3b9c0ae6baec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1]) tensor([10, 11])\n",
            "tensor([2, 3]) tensor([12, 13])\n",
            "tensor([4, 5]) tensor([14, 15])\n",
            "tensor([6, 7]) tensor([16, 17])\n",
            "tensor([8, 9]) tensor([18, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in DataLoader(dataset, batch_size=2, shuffle=True):\n",
        "    print(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIycTuTjfTVA",
        "outputId": "4a686e0c-7bf0-4649-b33a-e3d0d0f7c86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 7]) tensor([11, 17])\n",
            "tensor([9, 6]) tensor([19, 16])\n",
            "tensor([8, 4]) tensor([18, 14])\n",
            "tensor([3, 5]) tensor([13, 15])\n",
            "tensor([2, 0]) tensor([12, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "default_sampler = DataLoader(dataset).sampler\n",
        "for i in default_sampler:\n",
        "    # iterating over the SequentialSampler\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiMRpJtCfTYv",
        "outputId": "81934cda-0457-42c1-86c2-deaa8370131a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(default_sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLE6W5wqfTbU",
        "outputId": "6a283b41-6d53-49b0-e066-8395f8063240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.sampler.SequentialSampler"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import SequentialSampler\n",
        "sampler = SequentialSampler(dataset)\n",
        "for x in sampler:\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmXED6bQfTfU",
        "outputId": "ad055423-9087-45f1-9264-f6db3c103425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_sampler = DataLoader(dataset, shuffle=True).sampler\n",
        "for index in random_sampler:\n",
        "    print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9gjcL6hfTiD",
        "outputId": "a786850b-40d6-4763-d14b-bca791a07ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "5\n",
            "9\n",
            "7\n",
            "6\n",
            "0\n",
            "1\n",
            "4\n",
            "3\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(random_sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsTzThrGhiIO",
        "outputId": "a83209c1-c1ac-4a03-dc64-f9a796e0469e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.sampler.RandomSampler"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import RandomSampler\n",
        "random_sampler = RandomSampler(dataset)\n",
        "for x in random_sampler:\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c9KRKPIhiKM",
        "outputId": "d2fe433c-8494-4359-db45-d6c667b46f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "5\n",
            "1\n",
            "8\n",
            "6\n",
            "3\n",
            "2\n",
            "4\n",
            "7\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dl = DataLoader(dataset, sampler=random_sampler)\n",
        "for i in dl.sampler:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjJpPdr-hiNg",
        "outputId": "774ac980-a162-49ae-b07f-157d12b54aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "8\n",
            "7\n",
            "0\n",
            "1\n",
            "2\n",
            "6\n",
            "4\n",
            "9\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's say we want all batches in the first half to be separate from the second half... that's where batch_samplers come in."
      ],
      "metadata": {
        "id": "ip-IplpejA__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 3\n",
        "default_batch_sampler = DataLoader(dataset, batch_size=batch_size).batch_sampler\n",
        "for i, batch_indices in enumerate(default_batch_sampler):\n",
        "    print(f'Batch #{i} indices: ', batch_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Oc5zA_hieU",
        "outputId": "62a4b596-152d-44c6-a274-93801a4609be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch #0 indices:  [0, 1, 2]\n",
            "Batch #1 indices:  [3, 4, 5]\n",
            "Batch #2 indices:  [6, 7, 8]\n",
            "Batch #3 indices:  [9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(default_batch_sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2UvVQ_3hipN",
        "outputId": "928f9524-853b-4e80-a588-c534b2233f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.sampler.BatchSampler"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.sampler import BatchSampler"
      ],
      "metadata": {
        "id": "kgvagebThisw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(BatchSampler.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T8GPUrkkMGQ",
        "outputId": "9343f8bc-01be-4740-b6de-8bf85f756008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wraps another sampler to yield a mini-batch of indices.\n",
            "\n",
            "    Args:\n",
            "        sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
            "        batch_size (int): Size of mini-batch.\n",
            "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
            "            its size would be less than ``batch_size``\n",
            "\n",
            "    Example:\n",
            "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
            "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
            "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
            "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PARALLELISM"
      ],
      "metadata": {
        "id": "1c8qFopqbCPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Imports and parameters\n",
        "# ----------------------\n",
        "#\n",
        "# Import PyTorch modules and define parameters.\n",
        "#\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "input_size = 5\n",
        "output_size = 2\n",
        "\n",
        "batch_size = 30\n",
        "data_size = 100"
      ],
      "metadata": {
        "id": "rT8gIeqGmkeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################################\n",
        "# Device\n",
        "#\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "######################################################################\n",
        "# Dummy DataSet\n",
        "# -------------\n",
        "#\n",
        "# Make a dummy (random) dataset. You just need to implement the\n",
        "# getitem\n",
        "#\n",
        "\n",
        "class RandomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, size, length):\n",
        "        self.len = length\n",
        "        self.data = torch.randn(length, size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),\n",
        "                         batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "AA4YHpb_mkf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Simple Model\n",
        "# ------------\n",
        "#\n",
        "# For the demo, our model just gets an input, performs a linear operation, and\n",
        "# gives an output. However, you can use ``DataParallel`` on any model (CNN, RNN,\n",
        "# Capsule Net etc.)\n",
        "#\n",
        "# We've placed a print statement inside the model to monitor the size of input\n",
        "# and output tensors.\n",
        "# Please pay attention to what is printed at batch rank 0.\n",
        "#\n",
        "\n",
        "class Model(nn.Module):\n",
        "    # Our model\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.fc(input)\n",
        "        print(\"\\tIn Model: input size\", input.size(),\n",
        "              \"output size\", output.size())\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Vp6I6FVDmkjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Create Model and DataParallel\n",
        "# -----------------------------\n",
        "#\n",
        "# This is the core part of the tutorial. First, we need to make a model instance\n",
        "# and check if we have multiple GPUs. If we have multiple GPUs, we can wrap\n",
        "# our model using ``nn.DataParallel``. Then we can put our model on GPUs by\n",
        "# ``model.to(device)``\n",
        "#\n",
        "\n",
        "model = Model(input_size, output_size)\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "fs0KV-WM8ZMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Run the Model\n",
        "# -------------\n",
        "#\n",
        "# Now we can see the sizes of input and output tensors.\n",
        "#\n",
        "\n",
        "for data in rand_loader:\n",
        "    input = data.to(device)\n",
        "    output = model(input)\n",
        "    print(\"Outside: input size\", input.size(),\n",
        "          \"output_size\", output.size())"
      ],
      "metadata": {
        "id": "UFTwiXP7mkmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Results\n",
        "# -------\n",
        "#\n",
        "# If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as\n",
        "# expected. But if you have multiple GPUs, then you can get results like this."
      ],
      "metadata": {
        "id": "qt6BWDNHZl6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 GPUs\n",
        "# ~~~~~~\n",
        "#\n",
        "# If you have 2, you will see:\n",
        "#\n",
        "# .. code:: bash\n",
        "#\n",
        "#     # on 2 GPUs\n",
        "#     Let's use 2 GPUs!\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#         In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "#         In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
        "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])"
      ],
      "metadata": {
        "id": "rHA5eMZsZmHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 GPUs\n",
        "# ~~~~~~\n",
        "#\n",
        "# If you have 3 GPUs, you will see:\n",
        "#\n",
        "# .. code:: bash\n",
        "#\n",
        "#     Let's use 3 GPUs!\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#         In Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
        "#     Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([4, 5]) output size torch.Size([4, 2])\n",
        "#         In Model: input size torch.Size([2, 5]) output size torch.Size([2, 2])\n",
        "#     Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])"
      ],
      "metadata": {
        "id": "ykyoSOGVZmQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}